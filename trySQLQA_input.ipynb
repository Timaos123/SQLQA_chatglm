{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e99ef76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get \"http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json\": dial unix /var/run/docker.sock: connect: permission denied\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541fe82f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from elasticsearch import Elasticsearch\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "import tqdm\n",
    "\n",
    "# 初始化Elasticsearch\n",
    "es = Elasticsearch(hosts=[\"http://127.0.0.1:9200\"])\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# def encode_text(bert_model, bert_tokenizer, text):\n",
    "#     # encoding\n",
    "#     inputs = bert_tokenizer.encode_plus(\n",
    "#         text,\n",
    "#         add_special_tokens=True,\n",
    "#         padding=\"longest\",\n",
    "#         truncation=True,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "#     input_ids = inputs[\"input_ids\"].to(device)\n",
    "#     attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "#         embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "#     return embeddings\n",
    "\n",
    "def embed_text(text):\n",
    "    model = AutoModel.from_pretrained(\"./dependent_service/models--junnyu--roformer_chinese_sim_char_base\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"./dependent_service/models--junnyu--roformer_chinese_sim_char_base\",trust_remote_code=True)\n",
    "    input_ids = tokenizer(text, return_tensors='pt', truncation=True, padding=True)['input_ids']\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze(0).numpy().tolist()\n",
    "\n",
    "# def get_bert_embedding(myV):\n",
    "#     # embedding\n",
    "#     bert_model = AutoModel.from_pretrained(\"junnyu/roformer_chinese_sim_char_base\")\n",
    "#     bert_tokenizer = AutoTokenizer.from_pretrained(\"junnyu/roformer_chinese_sim_char_base\",trust_remote_code=True)\n",
    "#     query_vector=encode_text(bert_model, bert_tokenizer, myV)\n",
    "#     del bert_model\n",
    "#     del bert_tokenizer\n",
    "    \n",
    "#     return query_vector.flatten().tolist()\n",
    "\n",
    "def insert_into_es(df,es,index_name,id_col_name):\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"col_name\": {\"type\": \"keyword\"},\n",
    "                \"row_i\": {\"type\": \"integer\"},\n",
    "                \"id_col_name\": {\"type\": \"keyword\"},\n",
    "                \"id_col_name_val\": {\"type\": \"keyword\"},\n",
    "                \"value\": {\"type\": \"keyword\"},\n",
    "                \"col_name_vec\": {\"type\": \"dense_vector\", \"dims\": 768},  # 根据向量维度进行调整\n",
    "                \"value_vec\": {\"type\": \"dense_vector\", \"dims\": 768}  # 根据向量维度进行调整\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # 检查索引是否存在\n",
    "    index_exists = es.indices.exists(index=index_name)\n",
    "    # 如果索引不存在，则创建新索引\n",
    "    if not index_exists:\n",
    "        es.indices.create(index=index_name,body=mapping)\n",
    "        print(f\"索引 '{index_name}' 创建成功\")\n",
    "    else:\n",
    "        print(f\"索引 '{index_name}' 已存在\")\n",
    "    \n",
    "    for col in tqdm.tqdm(df.columns):\n",
    "        for i, cell in enumerate(df[col].values):\n",
    "            if pd.notna(cell) and isinstance(cell, str): # 值输入\n",
    "                # 拆分数据\n",
    "                col_name = col\n",
    "                row_i = i\n",
    "                id_col_name_val = df[id_col_name][i] if pd.notna(df[id_col_name][i]) else \"\"\n",
    "                value = cell\n",
    "\n",
    "                # 使用BERT嵌入向量化（伪代码）\n",
    "                col_name_vec = embed_text(col_name)\n",
    "                value_vec = embed_text(value)\n",
    "\n",
    "                # 构造文档数据\n",
    "                doc = {\n",
    "                    \"col_name\": col_name,\n",
    "                    \"row_i\": row_i,\n",
    "                    \"id_col_name\": id_col_name,\n",
    "                    \"id_col_name_val\": id_col_name_val,\n",
    "                    \"value\": value,\n",
    "                    \"col_name_vec\": col_name_vec,\n",
    "                    \"value_vec\": value_vec\n",
    "                }\n",
    "\n",
    "                # 将文档数据插入Elasticsearch索引\n",
    "                es.index(index=index_name, doc_type='_doc', body=doc)\n",
    "            else: # 列输入\n",
    "                # 拆分数据\n",
    "                col_name = col\n",
    "                row_i = i\n",
    "                id_col_name_val = df[id_col_name][i] if pd.notna(df[id_col_name][i]) else \"\"\n",
    "\n",
    "                # 使用BERT嵌入向量化（伪代码）\n",
    "                col_name_vec = embed_text(col_name)\n",
    "                value_vec = embed_text(value)\n",
    "\n",
    "                # 构造文档数据\n",
    "                doc = {\n",
    "                    \"col_name\": col_name,\n",
    "                    \"row_i\": row_i,\n",
    "                    \"id_col_name\": id_col_name,\n",
    "                    \"id_col_name_val\": id_col_name_val,\n",
    "                    \"value\": \"num\",\n",
    "                    \"col_name_vec\": col_name_vec,\n",
    "                    \"value_vec\": col_name_vec\n",
    "                }\n",
    "\n",
    "                # 将文档数据插入Elasticsearch索引\n",
    "                es.index(index=index_name, doc_type='_doc', body=doc)\n",
    "                \n",
    "                \n",
    "    print(\"数据已成功插入Elasticsearch索引：\", index_name)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    # 示例DataFrame\n",
    "#     df = pd.DataFrame({\n",
    "#         '学校': ['清华大学', '北京大学', '对外经济贸易大学'],\n",
    "#         '地址': ['北京', '湖南', '河北'],\n",
    "#         '学生数': [128, 211, 985]\n",
    "#     })\n",
    "    df = pd.read_excel(\"./data/fire.xlsx\")\n",
    "    # 拆分并构造Elasticsearch索引\n",
    "    index_name = \"try_fire_index\"  # 设置Elasticsearch索引的名称\n",
    "    id_col_name = \"主键\"  # 用户提供的id列名\n",
    "    \n",
    "    insert_into_es(df,es,index_name,id_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e1a8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from elasticsearch import Elasticsearch\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "import tqdm\n",
    "\n",
    "# 初始化Elasticsearch\n",
    "es = Elasticsearch(hosts=[\"http://127.0.0.1:9200\"])\n",
    "\n",
    "index_name = \"try_sql_index\"  # 设置Elasticsearch索引的名称\n",
    "es.indices.delete(index=index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "performpredict",
   "language": "python",
   "name": "performpredict"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
